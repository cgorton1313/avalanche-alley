{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86222ae",
   "metadata": {},
   "source": [
    "# NWS Prep\n",
    "This notebook provides download links for NWS data then ingests the data by station/year from html files.\n",
    "\n",
    "It handles cropping extra hours from the year (due to time zones) and some data clean-up.\n",
    "\n",
    "Finally, it outputs a pkl ready to be analyzed and used.\n",
    "\n",
    "---\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60da68",
   "metadata": {},
   "source": [
    "**Function to get appropriate file names and url's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaths(site, startYr):\n",
    "    endYr = startYr + 1\n",
    "    hours = '72'\n",
    "    units = 'english'\n",
    "    chart = 'off'\n",
    "    headers = 'none'\n",
    "    obs = 'tabular'\n",
    "    hourly = 'true'\n",
    "    pview = 'full'\n",
    "    history = 'yes'\n",
    "    start = str(startYr) + '0101'\n",
    "    end = str(endYr) + '0101'\n",
    "\n",
    "    url = f'https://www.weather.gov/wrh/timeseries?site={site}&hours={hours}&units={units}&chart={chart}&headers={headers}&obs={obs}&hourly={hourly}&pview={pview}&history={history}&start={start}&end={end}'\n",
    "\n",
    "    file_path = f'{site}-{startYr}.html'\n",
    "\n",
    "    return {'file_path':file_path,'url':url}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46b45f",
   "metadata": {},
   "source": [
    "## Get Each Site Wx Data\n",
    "\n",
    "This lists every link we must load, and what to save it as.\n",
    "\n",
    "Would love a programmatic way, but need async http calls..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d560a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earliest dates to pull:\n",
    "# JVEMT: 2019\n",
    "# S11MT: 2019\n",
    "# SNSLP: 2007, but no 2013-2014!\n",
    "# SH7MT: 2019\n",
    "# SH4MT: 2019\n",
    "# MRPMT: 2019\n",
    "\n",
    "sites_years = {'JVEMT': 2019, 'S11MT': 2019, 'SNSLP': 2007, 'SH7MT': 2019, 'SH4MT': 2019, 'MRPMT': 2019}\n",
    "files = set()\n",
    "\n",
    "for site, year in sites_years.items():    \n",
    "    for start in range(year, 2023):\n",
    "        paths = getPaths(site, start)\n",
    "        files.add(paths['file_path'])\n",
    "        print(paths['file_path'],'=',paths['url'],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709dd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNSLP 2013 has no data, must remove\n",
    "\n",
    "files.discard('SNSLP-2013.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade78577",
   "metadata": {},
   "source": [
    "**Create dictionary of sites (keys) with list (value) as tuple of file name and dataframe per year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: this cell takes 3 minutes to run!\n",
    "\n",
    "sites_wx = {'JVEMT': [], 'S11MT': [], 'SNSLP': [], 'SH7MT': [], 'SH4MT': [], 'MRPMT': []}\n",
    "\n",
    "for file in files:\n",
    "    print(f'Getting {file}')\n",
    "    df = pd.read_html('../raw_data/nws/' + file)[0] # b/c read_html gives a list of df's, and we have just one\n",
    "    df['site'] = file[:5] # parse site abbreviation from first 5 chars\n",
    "    df['file_name'] = file\n",
    "    sites_wx[file[:5]].append((file, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for site in sites_wx:\n",
    "  for yrs in sites_wx[site]:\n",
    "    df = yrs[1].copy().iloc[17:-7] # trim hours due to GMT\n",
    "    year = yrs[0][6:10]\n",
    "    df['year'] = year # parse from file name\n",
    "    dfs.append(df)\n",
    "\n",
    "all_wx = pd.concat(dfs)\n",
    "all_wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns, watch out for misordering due to file names in unordered set!\n",
    "new_cols = {\n",
    "    all_wx.columns[0]: \"dt\",\n",
    "    all_wx.columns[1]: \"temp\",\n",
    "    'DewPoint(°F)': \"dew_pt\",\n",
    "    'RelativeHumidity(%)': \"rH\",\n",
    "    'HeatIndex(°F)': \"heat_idx\",\n",
    "    'WindChill(°F)': \"wind_chill\",\n",
    "    'WindDirection': \"wind_dir\",\n",
    "    'WindSpeed(mph)' : \"wind_spd\",\n",
    "    'SnowDepth(in)': \"snow_depth\",\n",
    "    'Snowfall3 hour(in)': \"snowfall_3hr\",\n",
    "    'Snowfall6 Hour(in)': \"snowfall_6hr\",\n",
    "    'Snowfall24 Hour(in)': \"snowfall_24hr\",\n",
    "    'Sea LevelPressure(mb)': \"sea_lvl_press\",\n",
    "    'StationPressure(in Hg)': \"sta_press\",\n",
    "    'AltimeterSetting(in Hg)': \"altimeter_setting\",\n",
    "    'SolarRadiation(W/m²)': \"solar_radiation\",\n",
    "    'PercentPossible(%)': \"pct_possible\",\n",
    "    '1 HourPrecip(in)': \"hr_precip\"\n",
    "}\n",
    "\n",
    "all_wx.rename(columns=new_cols, inplace = True)\n",
    "all_wx.drop(columns='Unnamed: 1', inplace=True)\n",
    "all_wx.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05499909",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wx.dt = [f'{d}, {y}' for d, y in zip(all_wx.dt, all_wx.year)]\n",
    "all_wx.dt = pd.to_datetime(all_wx.dt, format='%b %d, %I:%M %p, %Y', errors='coerce') # parse the datetime\n",
    "all_wx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wx.reset_index(drop=True, inplace=True)\n",
    "all_wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "# profile = all_wx.profile_report()\n",
    "# profile.to_file('all_wx-profile.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4042377",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wx.to_pickle(\"pkl/all_wx.pkl\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "185e75714577421db51f68ec21bfca9e5e6dbad2e2dd32ea39a613a8c8024e3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
