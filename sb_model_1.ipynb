{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Machine Learning Model\n",
    "\n",
    "Sarah Braverman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When done, put the imputing and OHE code here to be able to create and use a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Purpose:** \n",
    "**A model to predict whether the avalanche will hit the rail depending on the conditions** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List: \n",
    "\n",
    "Do OHE, imputing etc. \n",
    "\n",
    "Test parameters, what are the best settings? Max_depth, n_estimators etc.\n",
    "\n",
    "Figure out how to find out which features impact the rail the most\n",
    " \n",
    "Train model on train data, fit on all data, train on all data, see how accurate it can be"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MG's Code is below, OHE is done, does not have imputing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avy_obs = pd.read_pickle('data-prep/pkl/avy_obs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = avy_obs.drop('deb_rail', axis=1)\n",
    "y = avy_obs['deb_rail']\n",
    "X = X.drop(['avcomment', 'edcr_user', 'edcr_ip',\n",
    "       'eded_user', 'eded_time', 'eded_ip'], axis=1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state = 1)\n",
    "cols_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "cols_missing_num = [col for col in X_train.columns if (X_train[col].dtypes != object) & (col in cols_missing)]\n",
    "\n",
    "cols_missing_num\n",
    "cols_missing_cat = [col for col in X_train.columns if (X_train[col].dtypes == object) & (col in cols_missing)]\n",
    "\n",
    "cols_missing_cat\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), cols_missing_cat))\n",
    "d = dict(zip(cols_missing_cat, object_nunique))\n",
    "\n",
    "sorted(d.items(), key=lambda x: x[1])\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[cols_missing_cat]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[cols_missing_cat]))\n",
    "\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "OH_cols_train.columns = OH_encoder.get_feature_names_out()\n",
    "OH_cols_valid.columns = OH_encoder.get_feature_names_out()\n",
    "\n",
    "num_X_train = X_train.drop(cols_missing_cat, axis=1)\n",
    "num_X_valid = X_valid.drop(cols_missing_cat, axis=1)\n",
    "\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82d6084fa52bbfe44e5d8bdc73d8f0ad601b91fd13e433e4ea3f469c7d7efe20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
